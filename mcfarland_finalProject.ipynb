{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use natural language processing on reviews to predict Airbnb scores? We will analyze 350,000 reviews on Airbnb listings to attempt to predict the components of an Airbnb score: accuracy, check in, communication, location, value, and overall score. \n",
    "\n",
    "Further, can our team do so in a generalizable way? We are breaking up this large dataset into thirds and working to predict our third as closely as possible...while making it generalizable and not overfit to this particular data. We will swap sets after our analysis is complete and see how closely we can predict the other portions of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import goslate             # language translation\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict \n",
    "import numpy as np\n",
    "from spellchecker import SpellChecker\n",
    "from enchant.checker import SpellChecker\n",
    "import time\n",
    "import enchant\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1_reviews_df = pd.read_csv(\"Airbnb - Chicago/reviews_chunk1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116838, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk1_reviews_df.shape # should only be first 116838 rows, 350674 is whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116838 entries, 0 to 116837\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             116838 non-null  int64 \n",
      " 1   listing_id     116838 non-null  int64 \n",
      " 2   date           116838 non-null  object\n",
      " 3   reviewer_id    116838 non-null  int64 \n",
      " 4   reviewer_name  116838 non-null  object\n",
      " 5   comments       116778 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "chunk1_reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team split 350674 dataset into thirds; my chunk of the data is rows 1 - 116838; row 0 is headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk1_reviews_df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk1_reviews_df.shape   # removed the blank rows at the end of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk1_reviews_df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 rows have null values in the comments. The following code will investigate these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>142113806</td>\n",
       "      <td>145690</td>\n",
       "      <td>4/6/2017</td>\n",
       "      <td>58511567</td>\n",
       "      <td>Aubrie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6886</th>\n",
       "      <td>247224472</td>\n",
       "      <td>397472</td>\n",
       "      <td>3/27/2018</td>\n",
       "      <td>180347596</td>\n",
       "      <td>Malik</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6969</th>\n",
       "      <td>367801717</td>\n",
       "      <td>397472</td>\n",
       "      <td>1/5/2019</td>\n",
       "      <td>209405594</td>\n",
       "      <td>Omowunmi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6982</th>\n",
       "      <td>419347754</td>\n",
       "      <td>397472</td>\n",
       "      <td>3/3/2019</td>\n",
       "      <td>141539770</td>\n",
       "      <td>Taje</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9534</th>\n",
       "      <td>151230808</td>\n",
       "      <td>585356</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>6991346</td>\n",
       "      <td>Dana</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13037</th>\n",
       "      <td>112574566</td>\n",
       "      <td>818109</td>\n",
       "      <td>11/6/2016</td>\n",
       "      <td>76764090</td>\n",
       "      <td>Gloria M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13417</th>\n",
       "      <td>179114636</td>\n",
       "      <td>872247</td>\n",
       "      <td>8/6/2017</td>\n",
       "      <td>85357888</td>\n",
       "      <td>Curt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13879</th>\n",
       "      <td>236627216</td>\n",
       "      <td>909096</td>\n",
       "      <td>2/19/2018</td>\n",
       "      <td>171974266</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14570</th>\n",
       "      <td>522147480</td>\n",
       "      <td>960326</td>\n",
       "      <td>9/1/2019</td>\n",
       "      <td>285528181</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19653</th>\n",
       "      <td>269591011</td>\n",
       "      <td>1428589</td>\n",
       "      <td>5/27/2018</td>\n",
       "      <td>187574744</td>\n",
       "      <td>Rohit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24772</th>\n",
       "      <td>152456464</td>\n",
       "      <td>1921670</td>\n",
       "      <td>5/16/2017</td>\n",
       "      <td>63649254</td>\n",
       "      <td>Meville</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>450774072</td>\n",
       "      <td>2730613</td>\n",
       "      <td>5/10/2019</td>\n",
       "      <td>259786341</td>\n",
       "      <td>Dianne</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33195</th>\n",
       "      <td>414751382</td>\n",
       "      <td>2879976</td>\n",
       "      <td>2/20/2019</td>\n",
       "      <td>223189372</td>\n",
       "      <td>Sanat</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33461</th>\n",
       "      <td>128085302</td>\n",
       "      <td>2907500</td>\n",
       "      <td>1/22/2017</td>\n",
       "      <td>111656608</td>\n",
       "      <td>Lo'Ay</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33896</th>\n",
       "      <td>256452329</td>\n",
       "      <td>2989466</td>\n",
       "      <td>4/22/2018</td>\n",
       "      <td>184798047</td>\n",
       "      <td>Marcellus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34658</th>\n",
       "      <td>331825032</td>\n",
       "      <td>3061036</td>\n",
       "      <td>10/3/2018</td>\n",
       "      <td>208346852</td>\n",
       "      <td>Yann</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35615</th>\n",
       "      <td>341763133</td>\n",
       "      <td>3138825</td>\n",
       "      <td>10/27/2018</td>\n",
       "      <td>147274808</td>\n",
       "      <td>Kim</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35875</th>\n",
       "      <td>164539866</td>\n",
       "      <td>3143254</td>\n",
       "      <td>6/28/2017</td>\n",
       "      <td>14698610</td>\n",
       "      <td>Cheng</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36455</th>\n",
       "      <td>342393779</td>\n",
       "      <td>3241929</td>\n",
       "      <td>10/28/2018</td>\n",
       "      <td>192542303</td>\n",
       "      <td>Katie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40936</th>\n",
       "      <td>238710264</td>\n",
       "      <td>3783533</td>\n",
       "      <td>2/26/2018</td>\n",
       "      <td>69200732</td>\n",
       "      <td>Mark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41454</th>\n",
       "      <td>208273478</td>\n",
       "      <td>3811926</td>\n",
       "      <td>10/31/2017</td>\n",
       "      <td>17405323</td>\n",
       "      <td>Christoph</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48115</th>\n",
       "      <td>227697993</td>\n",
       "      <td>4395112</td>\n",
       "      <td>1/15/2018</td>\n",
       "      <td>7309631</td>\n",
       "      <td>Murphy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49350</th>\n",
       "      <td>350158033</td>\n",
       "      <td>4668682</td>\n",
       "      <td>11/18/2018</td>\n",
       "      <td>180231282</td>\n",
       "      <td>Colin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54058</th>\n",
       "      <td>179534363</td>\n",
       "      <td>5480976</td>\n",
       "      <td>8/7/2017</td>\n",
       "      <td>1610028</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56567</th>\n",
       "      <td>332190590</td>\n",
       "      <td>5851115</td>\n",
       "      <td>10/4/2018</td>\n",
       "      <td>50601875</td>\n",
       "      <td>Benoit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58123</th>\n",
       "      <td>190611513</td>\n",
       "      <td>6014028</td>\n",
       "      <td>9/4/2017</td>\n",
       "      <td>148578546</td>\n",
       "      <td>Kelley</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58812</th>\n",
       "      <td>121149743</td>\n",
       "      <td>6065318</td>\n",
       "      <td>12/14/2016</td>\n",
       "      <td>94212764</td>\n",
       "      <td>Nanci</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62571</th>\n",
       "      <td>419416904</td>\n",
       "      <td>6447131</td>\n",
       "      <td>3/3/2019</td>\n",
       "      <td>246423354</td>\n",
       "      <td>Brandon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63114</th>\n",
       "      <td>102070794</td>\n",
       "      <td>6504170</td>\n",
       "      <td>9/16/2016</td>\n",
       "      <td>87580254</td>\n",
       "      <td>Matthew</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67287</th>\n",
       "      <td>138402999</td>\n",
       "      <td>6793653</td>\n",
       "      <td>3/19/2017</td>\n",
       "      <td>7260884</td>\n",
       "      <td>Scott</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68513</th>\n",
       "      <td>400571951</td>\n",
       "      <td>6953009</td>\n",
       "      <td>1/12/2019</td>\n",
       "      <td>234180486</td>\n",
       "      <td>Gerard</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78525</th>\n",
       "      <td>208689768</td>\n",
       "      <td>7810018</td>\n",
       "      <td>11/2/2017</td>\n",
       "      <td>127206805</td>\n",
       "      <td>Josh</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81227</th>\n",
       "      <td>483472643</td>\n",
       "      <td>8019077</td>\n",
       "      <td>7/7/2019</td>\n",
       "      <td>270755258</td>\n",
       "      <td>Clayton</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81889</th>\n",
       "      <td>338108028</td>\n",
       "      <td>8068975</td>\n",
       "      <td>10/18/2018</td>\n",
       "      <td>80351415</td>\n",
       "      <td>Garrett</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81963</th>\n",
       "      <td>501449867</td>\n",
       "      <td>8068975</td>\n",
       "      <td>8/3/2019</td>\n",
       "      <td>29557356</td>\n",
       "      <td>Minji</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84647</th>\n",
       "      <td>472874381</td>\n",
       "      <td>8256750</td>\n",
       "      <td>6/20/2019</td>\n",
       "      <td>260012976</td>\n",
       "      <td>Kristin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85348</th>\n",
       "      <td>314252727</td>\n",
       "      <td>8306409</td>\n",
       "      <td>8/26/2018</td>\n",
       "      <td>205345733</td>\n",
       "      <td>Rahul</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87713</th>\n",
       "      <td>154422220</td>\n",
       "      <td>8524146</td>\n",
       "      <td>5/24/2017</td>\n",
       "      <td>88005359</td>\n",
       "      <td>Thomas &amp; Eni</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88289</th>\n",
       "      <td>403705782</td>\n",
       "      <td>8581657</td>\n",
       "      <td>1/21/2019</td>\n",
       "      <td>237079955</td>\n",
       "      <td>Usama</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90884</th>\n",
       "      <td>459578849</td>\n",
       "      <td>8747106</td>\n",
       "      <td>5/27/2019</td>\n",
       "      <td>241144340</td>\n",
       "      <td>Paige</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92419</th>\n",
       "      <td>534424859</td>\n",
       "      <td>8827052</td>\n",
       "      <td>9/22/2019</td>\n",
       "      <td>64635875</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93299</th>\n",
       "      <td>207734673</td>\n",
       "      <td>9014565</td>\n",
       "      <td>10/29/2017</td>\n",
       "      <td>155811778</td>\n",
       "      <td>Megan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94479</th>\n",
       "      <td>452627007</td>\n",
       "      <td>9133069</td>\n",
       "      <td>5/13/2019</td>\n",
       "      <td>183219311</td>\n",
       "      <td>Guadalupe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94490</th>\n",
       "      <td>522988597</td>\n",
       "      <td>9133069</td>\n",
       "      <td>9/2/2019</td>\n",
       "      <td>42867566</td>\n",
       "      <td>Carlos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95920</th>\n",
       "      <td>191741852</td>\n",
       "      <td>9351265</td>\n",
       "      <td>9/8/2017</td>\n",
       "      <td>102524421</td>\n",
       "      <td>Trenton</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99607</th>\n",
       "      <td>210912040</td>\n",
       "      <td>9908933</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>114689465</td>\n",
       "      <td>Shriya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102607</th>\n",
       "      <td>528745611</td>\n",
       "      <td>10280848</td>\n",
       "      <td>9/13/2019</td>\n",
       "      <td>82554701</td>\n",
       "      <td>Anna-Marie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102657</th>\n",
       "      <td>274619734</td>\n",
       "      <td>10339986</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>126909742</td>\n",
       "      <td>Marian</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102664</th>\n",
       "      <td>455506188</td>\n",
       "      <td>10339986</td>\n",
       "      <td>5/19/2019</td>\n",
       "      <td>135738313</td>\n",
       "      <td>Dan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103292</th>\n",
       "      <td>416499922</td>\n",
       "      <td>10411427</td>\n",
       "      <td>2/24/2019</td>\n",
       "      <td>152507596</td>\n",
       "      <td>Utku</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104212</th>\n",
       "      <td>262693145</td>\n",
       "      <td>10548699</td>\n",
       "      <td>5/9/2018</td>\n",
       "      <td>74648805</td>\n",
       "      <td>Minh</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106210</th>\n",
       "      <td>416610400</td>\n",
       "      <td>10943002</td>\n",
       "      <td>2/24/2019</td>\n",
       "      <td>139892882</td>\n",
       "      <td>Jt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106400</th>\n",
       "      <td>503547269</td>\n",
       "      <td>10947455</td>\n",
       "      <td>8/6/2019</td>\n",
       "      <td>170374110</td>\n",
       "      <td>Jarius</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106690</th>\n",
       "      <td>428660947</td>\n",
       "      <td>10971626</td>\n",
       "      <td>3/25/2019</td>\n",
       "      <td>150610102</td>\n",
       "      <td>Cristian</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107042</th>\n",
       "      <td>305065193</td>\n",
       "      <td>10997225</td>\n",
       "      <td>8/10/2018</td>\n",
       "      <td>202744032</td>\n",
       "      <td>Mick</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111102</th>\n",
       "      <td>361031341</td>\n",
       "      <td>11476206</td>\n",
       "      <td>12/22/2018</td>\n",
       "      <td>133849216</td>\n",
       "      <td>Yuele</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111486</th>\n",
       "      <td>338461985</td>\n",
       "      <td>11510864</td>\n",
       "      <td>10/19/2018</td>\n",
       "      <td>208027292</td>\n",
       "      <td>Joe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112483</th>\n",
       "      <td>543482201</td>\n",
       "      <td>11637237</td>\n",
       "      <td>10/7/2019</td>\n",
       "      <td>39862531</td>\n",
       "      <td>Maire</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114615</th>\n",
       "      <td>416460440</td>\n",
       "      <td>11904373</td>\n",
       "      <td>2/24/2019</td>\n",
       "      <td>10641064</td>\n",
       "      <td>Adam</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115861</th>\n",
       "      <td>192094173</td>\n",
       "      <td>12037587</td>\n",
       "      <td>9/9/2017</td>\n",
       "      <td>104848689</td>\n",
       "      <td>Yash</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  listing_id        date  reviewer_id reviewer_name comments\n",
       "2771    142113806      145690    4/6/2017     58511567        Aubrie      NaN\n",
       "6886    247224472      397472   3/27/2018    180347596         Malik      NaN\n",
       "6969    367801717      397472    1/5/2019    209405594      Omowunmi      NaN\n",
       "6982    419347754      397472    3/3/2019    141539770          Taje      NaN\n",
       "9534    151230808      585356   5/11/2017      6991346          Dana      NaN\n",
       "13037   112574566      818109   11/6/2016     76764090      Gloria M      NaN\n",
       "13417   179114636      872247    8/6/2017     85357888          Curt      NaN\n",
       "13879   236627216      909096   2/19/2018    171974266         Sarah      NaN\n",
       "14570   522147480      960326    9/1/2019    285528181          Ryan      NaN\n",
       "19653   269591011     1428589   5/27/2018    187574744         Rohit      NaN\n",
       "24772   152456464     1921670   5/16/2017     63649254       Meville      NaN\n",
       "31967   450774072     2730613   5/10/2019    259786341        Dianne      NaN\n",
       "33195   414751382     2879976   2/20/2019    223189372         Sanat      NaN\n",
       "33461   128085302     2907500   1/22/2017    111656608         Lo'Ay      NaN\n",
       "33896   256452329     2989466   4/22/2018    184798047     Marcellus      NaN\n",
       "34658   331825032     3061036   10/3/2018    208346852          Yann      NaN\n",
       "35615   341763133     3138825  10/27/2018    147274808           Kim      NaN\n",
       "35875   164539866     3143254   6/28/2017     14698610         Cheng      NaN\n",
       "36455   342393779     3241929  10/28/2018    192542303         Katie      NaN\n",
       "40936   238710264     3783533   2/26/2018     69200732          Mark      NaN\n",
       "41454   208273478     3811926  10/31/2017     17405323     Christoph      NaN\n",
       "48115   227697993     4395112   1/15/2018      7309631        Murphy      NaN\n",
       "49350   350158033     4668682  11/18/2018    180231282         Colin      NaN\n",
       "54058   179534363     5480976    8/7/2017      1610028        Andrew      NaN\n",
       "56567   332190590     5851115   10/4/2018     50601875        Benoit      NaN\n",
       "58123   190611513     6014028    9/4/2017    148578546        Kelley      NaN\n",
       "58812   121149743     6065318  12/14/2016     94212764         Nanci      NaN\n",
       "62571   419416904     6447131    3/3/2019    246423354       Brandon      NaN\n",
       "63114   102070794     6504170   9/16/2016     87580254       Matthew      NaN\n",
       "67287   138402999     6793653   3/19/2017      7260884         Scott      NaN\n",
       "68513   400571951     6953009   1/12/2019    234180486        Gerard      NaN\n",
       "78525   208689768     7810018   11/2/2017    127206805          Josh      NaN\n",
       "81227   483472643     8019077    7/7/2019    270755258       Clayton      NaN\n",
       "81889   338108028     8068975  10/18/2018     80351415       Garrett      NaN\n",
       "81963   501449867     8068975    8/3/2019     29557356         Minji      NaN\n",
       "84647   472874381     8256750   6/20/2019    260012976       Kristin      NaN\n",
       "85348   314252727     8306409   8/26/2018    205345733         Rahul      NaN\n",
       "87713   154422220     8524146   5/24/2017     88005359  Thomas & Eni      NaN\n",
       "88289   403705782     8581657   1/21/2019    237079955         Usama      NaN\n",
       "90884   459578849     8747106   5/27/2019    241144340         Paige      NaN\n",
       "92419   534424859     8827052   9/22/2019     64635875        Daniel      NaN\n",
       "93299   207734673     9014565  10/29/2017    155811778         Megan      NaN\n",
       "94479   452627007     9133069   5/13/2019    183219311     Guadalupe      NaN\n",
       "94490   522988597     9133069    9/2/2019     42867566        Carlos      NaN\n",
       "95920   191741852     9351265    9/8/2017    102524421       Trenton      NaN\n",
       "99607   210912040     9908933  11/11/2017    114689465        Shriya      NaN\n",
       "102607  528745611    10280848   9/13/2019     82554701    Anna-Marie      NaN\n",
       "102657  274619734    10339986    6/9/2018    126909742        Marian      NaN\n",
       "102664  455506188    10339986   5/19/2019    135738313           Dan      NaN\n",
       "103292  416499922    10411427   2/24/2019    152507596          Utku      NaN\n",
       "104212  262693145    10548699    5/9/2018     74648805          Minh      NaN\n",
       "106210  416610400    10943002   2/24/2019    139892882            Jt      NaN\n",
       "106400  503547269    10947455    8/6/2019    170374110        Jarius      NaN\n",
       "106690  428660947    10971626   3/25/2019    150610102      Cristian      NaN\n",
       "107042  305065193    10997225   8/10/2018    202744032          Mick      NaN\n",
       "111102  361031341    11476206  12/22/2018    133849216         Yuele      NaN\n",
       "111486  338461985    11510864  10/19/2018    208027292           Joe      NaN\n",
       "112483  543482201    11637237   10/7/2019     39862531         Maire      NaN\n",
       "114615  416460440    11904373   2/24/2019     10641064          Adam      NaN\n",
       "115861  192094173    12037587    9/9/2017    104848689          Yash      NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk1_reviews_df[chunk1_reviews_df['comments'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 records out of 116,838 are null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = chunk1_reviews_df[chunk1_reviews_df['comments'].isnull()] # copy nulls values to their own dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397472      3\n",
       "9133069     2\n",
       "8068975     2\n",
       "10339986    2\n",
       "10947455    1\n",
       "8747106     1\n",
       "6793653     1\n",
       "6953009     1\n",
       "10943002    1\n",
       "3061036     1\n",
       "909096      1\n",
       "6065318     1\n",
       "9014565     1\n",
       "10411427    1\n",
       "8827052     1\n",
       "2989466     1\n",
       "6447131     1\n",
       "11637237    1\n",
       "8581657     1\n",
       "11904373    1\n",
       "5480976     1\n",
       "145690      1\n",
       "585356      1\n",
       "4668682     1\n",
       "3138825     1\n",
       "1921670     1\n",
       "8019077     1\n",
       "872247      1\n",
       "9908933     1\n",
       "818109      1\n",
       "3783533     1\n",
       "6504170     1\n",
       "2730613     1\n",
       "8524146     1\n",
       "8256750     1\n",
       "1428589     1\n",
       "2907500     1\n",
       "5851115     1\n",
       "10971626    1\n",
       "8306409     1\n",
       "4395112     1\n",
       "2879976     1\n",
       "7810018     1\n",
       "9351265     1\n",
       "10997225    1\n",
       "10548699    1\n",
       "3811926     1\n",
       "12037587    1\n",
       "3143254     1\n",
       "11510864    1\n",
       "6014028     1\n",
       "3241929     1\n",
       "11476206    1\n",
       "960326      1\n",
       "10280848    1\n",
       "Name: listing_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_df.listing_id.value_counts() # how many reviews are missing for each listing id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 60 listing ids with null values, most listings are only missing 1 review. One is missing 3 reviews, and 3 are missing 2 reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_ids = null_df.listing_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726376      603\n",
       "464581      594\n",
       "10069247    546\n",
       "2570620     514\n",
       "1171860     510\n",
       "           ... \n",
       "9576984       1\n",
       "11675937      1\n",
       "10729296      1\n",
       "10595206      1\n",
       "11046830      1\n",
       "Name: listing_id, Length: 1211, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk1_reviews_df.listing_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "histo = chunk1_reviews_df.listing_id.value_counts().hist()\n",
    "x= histo.set_xlabel(\"Numbers of Reviews per Listing\")\n",
    "y = histo.set_ylabel(\"Review Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of reviews have between 0-75 reviews, with very few listings with greater than 300 reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max number of null reviews per listing is 3. While that is significant for some listings, there is no way to recover the data. Therefore, all rows with missing comments will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116838 entries, 0 to 116837\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             116838 non-null  int64 \n",
      " 1   listing_id     116838 non-null  int64 \n",
      " 2   date           116838 non-null  object\n",
      " 3   reviewer_id    116838 non-null  int64 \n",
      " 4   reviewer_name  116838 non-null  object\n",
      " 5   comments       116778 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "chunk1_reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1_reviews_df.dropna(inplace=True) # drops rows with any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 116778 entries, 0 to 116837\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             116778 non-null  int64 \n",
      " 1   listing_id     116778 non-null  int64 \n",
      " 2   date           116778 non-null  object\n",
      " 3   reviewer_id    116778 non-null  int64 \n",
      " 4   reviewer_name  116778 non-null  object\n",
      " 5   comments       116778 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "chunk1_reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No other missing data, 116778 reviews remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25218143</td>\n",
       "      <td>2384</td>\n",
       "      <td>1/9/2015</td>\n",
       "      <td>14385014</td>\n",
       "      <td>Ivan</td>\n",
       "      <td>it's a wonderful trip experience. I didn't exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28475392</td>\n",
       "      <td>2384</td>\n",
       "      <td>3/24/2015</td>\n",
       "      <td>16241178</td>\n",
       "      <td>Namhaitou</td>\n",
       "      <td>This is my first trip using Airbnb. I was a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30273263</td>\n",
       "      <td>2384</td>\n",
       "      <td>4/19/2015</td>\n",
       "      <td>26101401</td>\n",
       "      <td>Patrick</td>\n",
       "      <td>The reservation was canceled 80 days before ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30974202</td>\n",
       "      <td>2384</td>\n",
       "      <td>4/30/2015</td>\n",
       "      <td>26247321</td>\n",
       "      <td>Cristina</td>\n",
       "      <td>Sólo puedo decir cosas buenas de Rebecca. La h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31363208</td>\n",
       "      <td>2384</td>\n",
       "      <td>5/4/2015</td>\n",
       "      <td>31293837</td>\n",
       "      <td>SuJung</td>\n",
       "      <td>Rebecca was an absolutely wonderful host.\\r\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116833</th>\n",
       "      <td>179042731</td>\n",
       "      <td>12159695</td>\n",
       "      <td>8/6/2017</td>\n",
       "      <td>14065150</td>\n",
       "      <td>Bel</td>\n",
       "      <td>Great host, clean apartment, and great locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116834</th>\n",
       "      <td>195267223</td>\n",
       "      <td>12159695</td>\n",
       "      <td>9/18/2017</td>\n",
       "      <td>148707949</td>\n",
       "      <td>Joshua</td>\n",
       "      <td>Great place! Super chill neighborhood. Check o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116835</th>\n",
       "      <td>333696318</td>\n",
       "      <td>12159695</td>\n",
       "      <td>10/7/2018</td>\n",
       "      <td>2980548</td>\n",
       "      <td>Christina</td>\n",
       "      <td>The house is just a block from the Blue line, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116836</th>\n",
       "      <td>448733096</td>\n",
       "      <td>12159695</td>\n",
       "      <td>5/5/2019</td>\n",
       "      <td>58521213</td>\n",
       "      <td>Adrian</td>\n",
       "      <td>Good place! Good value!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116837</th>\n",
       "      <td>547679964</td>\n",
       "      <td>12159695</td>\n",
       "      <td>10/15/2019</td>\n",
       "      <td>3709372</td>\n",
       "      <td>Mallaury</td>\n",
       "      <td>I had a lovely stay at Justine's place! She's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116778 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  listing_id        date  reviewer_id reviewer_name  \\\n",
       "0        25218143        2384    1/9/2015     14385014          Ivan   \n",
       "1        28475392        2384   3/24/2015     16241178     Namhaitou   \n",
       "2        30273263        2384   4/19/2015     26101401       Patrick   \n",
       "3        30974202        2384   4/30/2015     26247321      Cristina   \n",
       "4        31363208        2384    5/4/2015     31293837        SuJung   \n",
       "...           ...         ...         ...          ...           ...   \n",
       "116833  179042731    12159695    8/6/2017     14065150           Bel   \n",
       "116834  195267223    12159695   9/18/2017    148707949        Joshua   \n",
       "116835  333696318    12159695   10/7/2018      2980548     Christina   \n",
       "116836  448733096    12159695    5/5/2019     58521213        Adrian   \n",
       "116837  547679964    12159695  10/15/2019      3709372      Mallaury   \n",
       "\n",
       "                                                 comments  \n",
       "0       it's a wonderful trip experience. I didn't exc...  \n",
       "1       This is my first trip using Airbnb. I was a li...  \n",
       "2       The reservation was canceled 80 days before ar...  \n",
       "3       Sólo puedo decir cosas buenas de Rebecca. La h...  \n",
       "4       Rebecca was an absolutely wonderful host.\\r\\n\\...  \n",
       "...                                                   ...  \n",
       "116833  Great host, clean apartment, and great locatio...  \n",
       "116834  Great place! Super chill neighborhood. Check o...  \n",
       "116835  The house is just a block from the Blue line, ...  \n",
       "116836                            Good place! Good value!  \n",
       "116837  I had a lovely stay at Justine's place! She's ...  \n",
       "\n",
       "[116778 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk1_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dictionary to hold sentence tokens that pertain to each part of review\n",
    "def def_value(): \n",
    "    return []\n",
    "      \n",
    "# Defining the dict \n",
    "listing_comments = defaultdict(def_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True) # suppress scientific notation from np\n",
    "unique_listingIds = np.unique(chunk1_reviews_df.listing_id.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 116778 reviews are for 1211 unique listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_listingIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for listingId in unique_listingIds: # create default dicts for all the listing ids\n",
    "    listing_comments[listingId] = defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates sections to divide each review into the categories by which the reviews are grouped. There are six review categories: accuracy, check in, communication, location, value, and cleanliness with a 1 -10 scale. There is an overall review category that is on a 100 point scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_words=[\"accuracy\", \"truth\", \"inaccurate\", \"correct\", \"reality\", \"untrue\", \"describe\", \"described\",\"looks\",\\\n",
    "                \"pictures\",\"pictured\",\"description\",\"photos\",\"advertised\"]\n",
    "checkin_words= [\"check in\", \"met us\", \"keys\",\"check-in\"]\n",
    "communication_words=[\"communication\",\"contact\",\"response\",\"answer\",\"reply\",\"communicate\",\"messages\", \"message\",\"responsive\",\\\n",
    "                    \"helpful\",\"respond\",\"communicating\",\"communicates\",\"service\"]\n",
    "location_words=[\"location\",\"site\",\"locale\",\"locality\",\"close\",\"far\",\"spot\",\"bars\",\"train\",\"restaurants\",\"shopping\",\\\n",
    "                \"dining\",\"safe\",\"dangerous\",\"distance\", \"proximity\", \"across\",\"walk\",\"metro\",\"located\",\\\n",
    "                \"best places of chicago\",\"neighborhood\",\"downtown\",\"brown line\",\"pink line\",\"el station\",\"blue line\",\\\n",
    "                \"red line\",\"purple line\"]\n",
    "value_words=[\"value\",\"expensive\",\"cheap\",\"price\",\"budget\",\"fortune\",\"rate\", \"very near\",\"pricing\"]\n",
    "cleanliness_words = [\"clean\",\"dirty\",\"fresh\",\"filty\",\"spotless\",\"immaculate\", \"cleanliness\",\"tidy\",\"cockroach\",\"sparkling\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section does a bit of preprocessing by lowercasing all the text. It then groups the listings into the 6 categories. If the text doesn't fall into these categories, it is grouped into the 'unspecified' category. If a review is in a foreign language, it will also be grouped in that category to be translated and sorted into its correct category at a later point. The foreign language processing is very slow due to rate limits on the server on which the processing happens, so I chose not to do it on the entire set, only text that couldn't immediately be categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in chunk1_reviews_df.iterrows():\n",
    "    sentences = sent_tokenize(chunk1_reviews_df['comments'][index])\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()  # lowercase all text\n",
    "           \n",
    "        if any(word in sentence for word in accuracy_words):            \n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['accuracy'].append(sentence)\n",
    "                \n",
    "        elif any(word in sentence for word in checkin_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['checkin'].append(sentence)\n",
    "                \n",
    "        elif any(word in sentence for word in cleanliness_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['cleanliness'].append(sentence)\n",
    "            \n",
    "        elif any(word in sentence for word in communication_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['communication'].append(sentence)\n",
    "                \n",
    "        elif any(word in sentence for word in value_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['value'].append(sentence)\n",
    "            \n",
    "        elif any(word in sentence for word in location_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['location'].append(sentence)                    \n",
    "                                      \n",
    "        else:\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['unspecified'].append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listing_comments) # all sentences in listing reviews have been sorted by review category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization requires a part of speech to work correctly, otherwise all words are assumed to be nouns. When the words are tokenized, it gets tagged with a part of speech by the context. However, that part of speech is more complex than we need for our current purposes. This maps from the complex speech parts to the simpler NOUN = 'n', ADJ = 's', VERB = 'v' and ADV = 'r' that is required for the nltk lemmatizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map between Treebank and WordNet \n",
    "# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'\n",
    "# Descriptions (c) https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
    "tag_map = {\n",
    "        'CC':None, # coordin. conjunction (and, but, or)  \n",
    "        'CD':wn.NOUN, # cardinal number (one, two)             \n",
    "        'DT':None, # determiner (a, the)                    \n",
    "        'EX':wn.ADV, # existential ‘there’ (there)           \n",
    "        'FW':None, # foreign word (mea culpa)             \n",
    "        'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "        'JJ':wn.ADJ, # adjective (yellow)      \n",
    "        'JJR':wn.ADJ, # adj., comparative (bigger)          \n",
    "        'JJS':wn.ADJ, # adj., superlative (wildest)           \n",
    "        'LS':None, # list item marker (1, 2, One)          \n",
    "        'MD':None, # modal (can, should)                    \n",
    "        'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "        'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "        'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "        'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "        'PDT':wn.ADJ, # predeterminer (all, both)            \n",
    "        'POS':None, # possessive ending (’s )               \n",
    "        'PRP':None, # personal pronoun (I, you, he)     \n",
    "        'PRP$':None, # possessive pronoun (your, one’s)    \n",
    "        'RB':wn.ADV, # adverb (quickly, never)            \n",
    "        'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "        'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "        'RP':wn.ADJ, # particle (up, off)\n",
    "        'SYM':None, # symbol (+,%, &)\n",
    "        'TO':None, # “to” (to)\n",
    "        'UH':None, # interjection (ah, oops)\n",
    "        'VB':wn.VERB, # verb base form (eat)\n",
    "        'VBD':wn.VERB, # verb past tense (ate)\n",
    "        'VBG':wn.VERB, # verb gerund (eating)\n",
    "        'VBN':wn.VERB, # verb past participle (eaten)\n",
    "        'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "        'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "        'WDT':None, # wh-determiner (which, that)\n",
    "        'WP':None, # wh-pronoun (what, who)\n",
    "        'WP$':None, # possessive (wh- whose)\n",
    "        'WRB':None, # wh-adverb (how, where)\n",
    "        '$':None, #  dollar sign ($)\n",
    "        '#':None, # pound sign (#)\n",
    "        '“':None, # left quote (‘ or “)\n",
    "        '”':None, # right quote (’ or ”)\n",
    "        '(':None, # left parenthesis ([, (, {, <)\n",
    "        ')':None, # right parenthesis (], ), }, >)\n",
    "        ',':None, # comma (,)\n",
    "        '.':None, # sentence-final punc (. ! ?)\n",
    "        ':':None # mid-sentence punc (: ; ... – -)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/ Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].\n",
    "\n",
    "Subjectivity was increased by 1.5 for positive reviews and decreased by 0.5 for negative reviews. The logic behind this was that if a person feels strongly about a place they will use more adjectives, which are words that score higher in the subjectivity score. Based on initial results, it appeared that people's ratings were higher than then their sentiment analysis registered. This may be that people are reluctant to give poor scores despite their experience, or that it is difficult to capture the extent to which a person/group enjoyed a place based on their sentiment. The +1.5  and -0.5 were adjusted to help the results fit the reviews in my training set. The real trial of this will be if it continues to fit in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_calculator(subjectivityscore):\n",
    "    if subjectivityscore[0]>=0: # if polarity is positive, shift subjectivity score and increase overall score by that amt\n",
    "        return((subjectivityscore[0]/2*10+5)+(subjectivityscore[1]+1.5)) # shift polarity scale from -1 to 1 to 1 to 10\n",
    "    else: # if polarity is negative, shift subjectivity score and increase overall score by that amt\n",
    "        return((subjectivityscore[0]/2*10+5)+(subjectivityscore[1]-0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to normalize each set of words. It performs spell correction, removes punctuation, creates tokens out of each word, and then finally lemmatizes each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_listing_text(listing_dict):\n",
    "    processed_listing=[]\n",
    "    for sentence in listing_dict:\n",
    "        sentence = str(TextBlob(sentence).correct()) # use TextBlob spelling correction\n",
    "        sentence = re.sub('[^a-zA-Z\\s\\'\\`]', \"\", sentence) # remove punctuation from sentences        \n",
    "            \n",
    "        tokens = word_tokenize(sentence) # Generate list of tokens\n",
    "        tokens_pos = pos_tag(tokens) \n",
    "        tok = \"\"\n",
    "        for token in tokens_pos:\n",
    "            if token != \"''\" and token[1] != \"''\" and token[1] != '``':\n",
    "                if tag_map[token[1]] == wn.NOUN or tag_map[token[1]] == None:\n",
    "                    tok = 'n'\n",
    "                elif tag_map[token[1]] == wn.VERB:\n",
    "                    tok = 'v'\n",
    "                elif tag_map[token[1]] == wn.ADJ:\n",
    "                    tok = 's'\n",
    "                elif tag_map[token[1]] == wn.ADV:\n",
    "                    tok = 'r'\n",
    "                processed_listing.append(lemmatizer.lemmatize(token[0],tok))\n",
    "            processed_listing.append(token[0])\n",
    "    return(processed_listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in unique_listingIds:\n",
    "    for sentence in listing_comments[ids]['unspecified']:\n",
    "        #if len(sentence) > 2:   # must have 3+ characters in string for lang detect\n",
    "            #blob = TextBlob(sentence) \n",
    "            #if blob.detect_language() != 'en': # translate to english\n",
    "            #    try:\n",
    "            #        sentence = blob.translate(to= 'en')                    \n",
    "            #    except:\n",
    "            #        print(sentence + \" not translated\")\n",
    "            #        continue # skip to next sentence\n",
    "            #time.sleep(5)  # avoid rate limit in TextBlob             \n",
    "            \n",
    "        # many words from different languages ended up in the unspecified dict; after translation, re-sort\n",
    "        if any(word in sentence for word in accuracy_words):            \n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['accuracy'].append(sentence)\n",
    "        elif any(word in sentence for word in checkin_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['checkin'].append(sentence)\n",
    "        elif any(word in sentence for word in cleanliness_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['cleanliness'].append(sentence)\n",
    "        elif any(word in sentence for word in communication_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['communication'].append(sentence)\n",
    "        elif any(word in sentence for word in value_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['value'].append(sentence)\n",
    "        elif any(word in sentence for word in location_words):\n",
    "            listing_comments[chunk1_reviews_df[\"listing_id\"][index]]['location'].append(sentence)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_wordbag_bylisting = defaultdict(def_value) \n",
    "for ids in unique_listingIds:\n",
    "    unspecified_wordbag_bylisting[ids] = process_listing_text(listing_comments[ids]['unspecified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unspecified_wordbag_bylisting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_wordbag_bylisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unspecified_bylisting = defaultdict(def_value)\n",
    "ss_unspec_bylisting=defaultdict(def_value)\n",
    "unspecified_score=defaultdict(def_value)\n",
    "                             \n",
    "for id in unspecified_wordbag_bylisting:\n",
    "    filtered_unspecified_bylisting[id] = [w for w in unspecified_wordbag_bylisting[id] if not w in stop_words]       \n",
    "\n",
    "for ind in filtered_unspecified_bylisting: # get the sentiment on all words in the list per listing id\n",
    "    ss_unspec_bylisting[ind] = TextBlob(\" \".join(filtered_unspecified_bylisting[ind])).sentiment \n",
    "\n",
    "for ind in ss_unspec_bylisting:\n",
    "    unspecified_score[ind] = score_calculator(ss_unspec_bylisting[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_wordbag_bylisting = defaultdict(def_value) \n",
    "for ids in unique_listingIds:\n",
    "    accuracy_wordbag_bylisting[ids] = process_listing_text(listing_comments[ids]['accuracy'])  \n",
    "    print(ids, accuracy_wordbag_bylisting[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accuracy_wordbag_bylisting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " filtered_accuracy_bylisting = defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in accuracy_wordbag_bylisting:\n",
    "    filtered_accuracy_bylisting[id] = [w for w in accuracy_wordbag_bylisting[id] if not w in stop_words]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_accuracy_bylisting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_accuracy_bylisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in filtered_accuracy_bylisting:\n",
    "    fd = FreqDist(filtered_accuracy_bylisting[ind])\n",
    "    #fd.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_acc_bylisting=defaultdict(def_value)\n",
    "accuracy_score=defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in filtered_accuracy_bylisting: # get the sentiment on all words in the list per listing id\n",
    "    ss_acc_bylisting[ind] = TextBlob(\" \".join(filtered_accuracy_bylisting[ind])).sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in ss_acc_bylisting:\n",
    "    accuracy_score[ind] = score_calculator(ss_acc_bylisting[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_bylisting = defaultdict(def_value)  \n",
    "filtered_checkin_bylisting = defaultdict(def_value)  \n",
    "ss_checkin_bylisting=defaultdict(def_value)\n",
    "checkin_score=defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in unique_listingIds:\n",
    "    checkin_bylisting[ids] = process_listing_text(listing_comments[ids]['checkin'])                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in checkin_bylisting:\n",
    "    filtered_checkin_bylisting[ids] = [w for w in checkin_bylisting[ids] if not w in stop_words]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in filtered_checkin_bylisting: # get the sentiment on all words in the list per listing id\n",
    "    ss_checkin_bylisting[ind] = TextBlob(\" \".join(filtered_checkin_bylisting[ind])).sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_comments[ids]['checkin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in ss_checkin_bylisting:\n",
    "    checkin_score[ind] = score_calculator(ss_checkin_bylisting[ind])\n",
    "\n",
    "checkin_score         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanliness_bylisting = defaultdict(def_value)  \n",
    "filtered_cleanliness_bylisting = defaultdict(def_value)  \n",
    "ss_clean_bylisting=defaultdict(def_value)\n",
    "cleanliness_score=defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing_comments['listing_id']['index']['cleanliness']\n",
    "for ids in unique_listingIds:\n",
    "    cleanliness_bylisting[ids] = process_listing_text(listing_comments[ids]['cleanliness'])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in unspecified_wordbag_bylisting:\n",
    "    filtered_cleanliness_bylisting[id] = [w for w in cleanliness_bylisting[id] if not w in stop_words]       \n",
    "\n",
    "for ind in filtered_cleanliness_bylisting: # get the sentiment on all words in the list per listing id\n",
    "    ss_clean_bylisting[ind] = TextBlob(\" \".join(filtered_cleanliness_bylisting[ind])).sentiment \n",
    "\n",
    "for ind in ss_clean_bylisting:\n",
    "    cleanliness_score[ind] = score_calculator(ss_clean_bylisting[ind])\n",
    "\n",
    "cleanliness_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_bylisting = defaultdict(def_value)  \n",
    "filtered_comm_bylisting = defaultdict(def_value)  \n",
    "ss_comm_bylisting=defaultdict(def_value)\n",
    "comm_score=defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing_comments['listing_id']['index']['communication']\n",
    "for ids in unique_listingIds:\n",
    "    comm_bylisting[ids] = process_listing_text(listing_comments[ids]['communication'])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in unspecified_wordbag_bylisting:\n",
    "    filtered_comm_bylisting[id] = [w for w in comm_bylisting[id] if not w in stop_words]       \n",
    "\n",
    "for ind in filtered_unspecified_bylisting: # get the sentiment on all words in the list per listing id\n",
    "    ss_comm_bylisting[ind] = TextBlob(\" \".join(filtered_comm_bylisting[ind])).sentiment \n",
    "\n",
    "for ind in ss_unspec_bylisting:\n",
    "    comm_score[ind] = score_calculator(ss_comm_bylisting[ind])\n",
    "\n",
    "comm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_bylisting = defaultdict(def_value)  \n",
    "filtered_value_bylisting = defaultdict(def_value)  \n",
    "ss_value_bylisting=defaultdict(def_value)\n",
    "value_score=defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing_comments['listing_id']['index']['value']\n",
    "for ids in unique_listingIds:\n",
    "    value_bylisting[ids] = process_listing_text(listing_comments[ids]['value'])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in value_bylisting:\n",
    "    filtered_value_bylisting[id] = [w for w in value_bylisting[id] if not w in stop_words]       \n",
    "\n",
    "for ind in filtered_value_bylisting: # get the sentiment on all words in the list per listing id\n",
    "    ss_value_bylisting[ind] = TextBlob(\" \".join(filtered_value_bylisting[ind])).sentiment \n",
    "\n",
    "for ind in ss_unspec_bylisting:\n",
    "    value_score[ind] = score_calculator(ss_value_bylisting[ind])\n",
    "\n",
    "value_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_bylisting = defaultdict(def_value)  \n",
    "filtered_location_bylisting = defaultdict(def_value)  \n",
    "ss_loc_bylisting=defaultdict(def_value)\n",
    "location_score=defaultdict(def_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing_comments['listing_id']['index']['location']\n",
    "for ids in unique_listingIds:\n",
    "    location_bylisting[ids] = process_listing_text(listing_comments[ids]['location'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in location_bylisting:\n",
    "    filtered_location_bylisting[id] = [w for w in location_bylisting[id] if not w in stop_words]       \n",
    "\n",
    "for ind in filtered_unspecified_bylisting: # get the sentiment on all words in the list per listing id\n",
    "    ss_loc_bylisting[ind] = TextBlob(\" \".join(filtered_location_bylisting[ind])).sentiment \n",
    "\n",
    "for ind in ss_unspec_bylisting:\n",
    "    location_score[ind] = score_calculator(ss_loc_bylisting[ind])\n",
    "\n",
    "location_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = {'AccuracyScore_calc': accuracy_score, 'CleanScore_calc':cleanliness_score, 'CheckinScore_calc':checkin_score, 'CommScore_calc':comm_score, 'LocationScore_calc':location_score, 'ValueScore_Calc':value_score}\n",
    "review_scores_df = pd.DataFrame(data=dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_scores_df.to_csv('review_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
